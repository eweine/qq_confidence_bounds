{
    "collab_server" : "",
    "contents" : "\\documentclass[article]{jss}\n\\usepackage{thumbpdf,lmodern}\n\\usepackage[utf8]{inputenc}\n\\usepackage[margin=1.25in]{geometry}\n\\usepackage{amsmath}\n\\usepackage{amssymb}\n\\usepackage{graphicx}% http://ctan.org/pkg/graphicx\n\\usepackage{dsfont}\n\\usepackage{tikz}\n\\usepackage{algorithm}\n\\usepackage{algorithmic}\n\\usepackage{eqparbox}\n\\renewcommand{\\algorithmiccomment}[1]{\\hfill\\eqparbox{COMMENT}{\\# #1}}\n\\makeatletter\n\\newcommand{\\class}[1]{`\\code{#1}'}\n\\newcommand{\\fct}[1]{\\code{#1()}}\n\\newcommand{\\distas}[1]{\\mathbin{\\overset{#1}{\\kern\\z@\\sim}}}%\n\\newsavebox{\\mybox}\\newsavebox{\\mysim}\n\\newcommand{\\distras}[1]{%\n  \\savebox{\\mybox}{\\hbox{\\kern3pt$\\scriptstyle#1$\\kern3pt}}%\n  \\savebox{\\mysim}{\\hbox{$\\sim$}}%\n  \\mathbin{\\overset{#1}{\\kern\\z@\\resizebox{\\wd\\mybox}{\\ht\\mysim}{$\\sim$}}}%\n}\n\\makeatother\n\n\\SweaveOpts{engine=R, eps=FALSE, keep.source = TRUE}\n<<preliminaries, echo=FALSE, results=hide>>=\noptions(prompt = \"R> \", continue = \"+  \", width = 70, useFancyQuotes = FALSE)\n@\n\n\\author{Eric Weine\\\\University of Chicago\n   \\And Mary Sara McPeek\\\\University of Chicago}\n\\Plainauthor{Eric Weine, Mary Sara McPeek}\n\n\\title{Efficient Calculation of Improved Simultaneous Testing Bands for QQplots}\n%\\author{Eric Weine and Mary Sara McPeek}\n\\Abstract{\nQuantile-Quantile plots (QQplots) are often difficult to interpret because it is unclear how large the deviation from the theoretical distribution must be to indicate a lack of fit. Current packages and algorithms towards this end either do not ensure a robust Type I error rate, are too slow, or are under-powered to deviations in the tails of the distribution. In this paper, we present an efficient algorithm that computes simultaneous testing bands for QQplots. These bands have a variety of desirable properties, including being fast to compute and being sensitive in the tails of the null distribution.\n}\n\n\\Keywords{QQplots, Equal Local Levels, Kolmolgorov-Smirnov, GWAS, Multiple Testing}\n\n\\Address{\n  Mary Sara McPeek\\\\\n  Department of Statistics\\\\\n  University of Chicago\\\\\n  924 E 57th St., Chicago, IL\\\\\n  E-mail: \\email{mcpeek@uchicago.edu}\\\\\n  URL: \\url{https://galton.uchicago.edu/~mcpeek/}\n}\n\n\\begin{document}\n\n\\maketitle\n\n\\section{Introduction}\nQuantile-Quantile plots (QQplots) are a common statistical tool used to judge if a sample comes from a specified distribution \\citep{wilk_gnanadesikan_1968}. Despite their ubiquity, they are often difficult to interpret because it is challenging to determine if the magnitude of the deviation from the specified distribution is large enough to indicate a lack of fit as opposed to just being the result of sampling variance. To make this determination, it is useful to put testing bands on a QQplot.\n\\newline\n\\newline\nA number of methods have been created towards this end. First, some available software puts pointwise testing bands onto QQplots that conduct $\\alpha$ level tests on each order statistic \\citep{qqplotr}. This method is clearly insufficient, as it ignores the multiple testing problem and thus provides no guarantee on the Type I error rate of testing the global hypothesis that all of the data come from some distribution. Second, the Komolgorov-Smirnov (KS) test is a common method for creating joint bands \\citep{kolmogoroff1941confidence, smirnov1944approximate}. While this method ensures the correct Type I error by using the distribution of the KS statistic, this test suffers from very low power under a variety of reasonable alternatives \\citep{aldor2013power}. In the same work, Aldor-Noiman et al. introduced a much more powerful test (under most alternatives) called the Tail Sensitive (TS) test. This test uses simulation to conduct an $\\eta$ level test on each order statistic such that the overall test has an $\\alpha$ level Type I error. This test has a variety of desirable properties, but because it requires simulation it is extremely slow in large datasets.\n\\newline\n\\newline\nIn what follows, we present a method that yields the same results as the TS test but does so substantially faster because of an efficient recursive calculation. We then demonstrate this test on a few multiple testing scenarios including Type I error calibration and genetic studies.\n\\section{Methods}\n\n\\subsection{Local Levels For Global Null Hypothesis Testing}\n\nSuppose we have observations\n\n\\begin{equation*}\n  X_{1}, ..., X_{n} \\distras{iid} F,\n\\end{equation*}\nand we are interested in conducting the following hypothesis test\n\n\\begin{equation*}\n      H_{0}: F = F_{0} \\textrm{ vs. } H_{A}: F \\neq F_{0}\n\\end{equation*}\nwith level $\\alpha$, where all parameters of $F_{0}$ are known. One approach to this problem, referred to as ``local levels,'' is to conduct separate hypothesis tests on each of the order statistics $X_{(i)}$, where each test has Type I error rate $\\eta_{i}$ \\citep{gontscharuk2016goodness}. Then, we reject the global null hypothesis if at least one of the $n$ tests results in a rejection. That is, we construct a set of intervals\n\n\\begin{equation*}\n    (h_{1}, g_{1}), ..., (h_{n}, g_{n}),\n\\end{equation*}\nand we reject $H_{0}$ if for any $i$\n\n\\begin{equation*}\n    X_{(i)} \\not\\in (h_{i}, g_{i}).\n\\end{equation*}\nIf we know the local level $\\eta_{i}$, then $h_{i} = F^{-1}_{i}(\\eta_{i} / 2)$ and $g_{i} = F^{-1}_{i}(1 - \\eta_{i} / 2)$, where $F_{i}$ is the CDF of the order statistics. Thus, the difficulty is in determining each $\\eta_{i}$. In our case, we want to create testing bands that are ``agnostic'' to any alternative distribution. By this, we mean that we would like to design a test that applies equal scrutiny to each order statistic, and thus we set\n% an alternative explanation is that we're trying to design a test that would have optimum power under the alternative distribution where the probability of any of the order statistics being outside of the pointwise null region is equal.\n\\begin{equation*}\n    \\eta_{1} = \\eta_{2} = ... = \\eta_{n} = \\eta.\n\\end{equation*}\n\nThis is known as ``equal local levels.''\n\\newline\n\\newline\n\\noindent Given the sample size $n$ and the desired level $\\alpha$ we present the following algorithm for obtaining $\\eta$ in two steps:\n\\newline\n\\newline\n1. Given a proposed testing region $(h_{1}, g_{1}), ..., (h_{n}, g_{n})$, find the probability of at least one of $X_{(1)}, ..., X_{(n)}$ falling outside of this region under the null.\n\\newline\n\\newline\n2. Using the algorithm described in step 1, conduct a binary search on the space of the local level $\\eta$. This involves proposing a value of $\\eta$, calculating the corresponding region $(h_{1}, g_{1}), ..., (h_{n}, g_{n})$ using $F_{i}^{-1}$ as described above, and then using step 1 to check if the region achieves the desired global Type I error rate $\\alpha$.\n\\newline\n\\newline\nWe now describe these two steps in more detail:\n\\newline\n\\newline\n\\textbf{Step 1:} First, note that under the null hypothesis,\n\n\\begin{equation*}\n    F_{0}(X_{1}), ..., F_{0}(X_{n}) \\distras{iid} Unif(0, 1).\n\\end{equation*}\nThus, we build the algorithm for step 1 assuming that\n\n\\begin{equation*}\n    X_{1}, ..., X_{n} \\distras{iid} Unif(0, 1)\n\\end{equation*}\n% I'll worry about the uncertainty in parameter estimates later\n% I'll probably want some sort of a diagram here eventually\nWe are trying to calculate the following probability\n\\begin{equation*}\n    \\alpha = P_{0}\\Big(\\bigcup\\limits_{i=1}^{n} \\{X_{(i)} \\notin (h_{i}, g_{i})\\}\\Big)\n\\end{equation*}\nWhere $P_{0}$ is the probability under the null hypothesis. It is easier to calculate this as\n\\begin{equation*}\n    \\alpha = 1 - P_{0}\\Big(\\bigcap\\limits_{i=1}^{n} \\{X_{(i)} \\in (h_{i}, g_{i})\\}\\Big)\n\\end{equation*}\n\\newline\nTo calculate this value, it is easier to transform the events over order statistics to multinomial events. To do this, we introduce the following notation:\n\\newline\n\\newline\nLet $b_{1}, ..., b_{2n}$ be the ordered values of $h_{1}, ..., h_{n}, g_{1}, ..., g_{n}$. We also define $b_{0} = 0$ for notational convenience. We will divide the interval $(b_{0}, b_{2n})$ into $2n$ bins, where bin 1 is $(b_{0}, b_{1})$, bin 2 is $(b_{1}, b_{2}), ...,$ and bin $2n$ is $(b_{2n - 1}, b_{2n})$. Let $N_{j} = \\sum_{i = 1}^{n}\\mathds{1}\\Big(X_{i} \\in (b_{j - 1}, b_{j})\\Big)$ denote the random variable that counts the number of X's falling into bin j, and let $S_{k} =  \\sum_{j=1}^{k} N_{j}$ be the $kth$ partial sum of the $N$'s. The key to the algorithm is that the following two events are the same:\n\\begin{equation*}\n    \\{X_{(i)} \\in (h_{i}, g_{i}) \\text{ for } i = 1, ..., n\\} = \\{l_{k} \\leq S_{k} \\leq u_{k} \\textrm{ for } k = 1,..., 2n\\},\n\\end{equation*}\nwhere\n\n\\begin{align*}\nu_{k} &=\n    \\begin{cases}\n      0, & \\text{if}\\ k=1 \\\\\n      \\sum_{i = 1}^{k - 1} \\mathds{1} \\big(b_{i} \\in \\{h_{1}, ..., h_{n}\\}\\big), & \\text{otherwise}\n    \\end{cases}\\\\\n    l_{k} &=\n      \\sum_{i = 1}^{k} \\mathds{1} \\big(b_{i} \\in \\{g_{1}, ..., g_{n}\\}\\big)\n\\end{align*}\n(See Figure 1 below for an example).\n% I should give some intuition here with a diagram or a better explanation\n\\begin{figure}\n\\centering\n\\begin{tikzpicture}\n\\draw (0,0) --(12.5,0);\n\\draw (0,-.5) --(0,.5);\n\\node at (0,.75) {0};\n\\draw (12.5,-.5) --(12.5,.5);\n\\node at (12.5,.75) {1};\n\\draw (1.5,-.5) --(1.5,.5);\n\\node at (1.5,.75) {h1};\n\\draw (5.25,-.5) --(5.25,.5);\n\\node at (5.25,.75) {g1};\n\\draw (9,-.5) --(9,.5);\n\\node at (9,.75) {h3};\n\\draw (3.5,-.5) --(3.5,.5);\n\\node at (3.5,.75) {h2};\n\\draw (7.25,-.5) --(7.25,.5);\n\\node at (7.25,.75) {g2};\n\\draw (11,-.5) --(11,.5);\n\\node at (11,.75) {g3};\n\\node at (.75,.25) {$u_{1} = 0$};\n\\node at (.75,-.25) {$l_{1} = 0$};\n\\node at (2.5,.25) {$u_{2} = 1$};\n\\node at (2.5,-.25) {$l_{2} = 0$};\n\\node at (4.35,.25) {$u_{3} = 2$};\n\\node at (4.35,-.25) {$l_{3} = 1$};\n\\node at (6.25,.25) {$u_{4} = 2$};\n\\node at (6.25,-.25) {$l_{4} = 2$};\n\\node at (8.15,.25) {$u_{5} = 2$};\n\\node at (8.15,-.25) {$l_{5} = 2$};\n\\node at (10,.25) {$u_{6} = 3$};\n\\node at (10,-.25) {$l_{6} = 3$};\n\\node at (11.75,.25) {$.\\;.\\;.$};\n\\end{tikzpicture}\n\\caption{Example of how $l_{k}$ and $u_{k}$ change with the arrangement of bounds. }\n\\end{figure}\n\\newline\nIntuitively, $u_{k}$ is the number of intervals $(h_{i}, g_{i})$ that have been opened before $b_{k}$, and $l_{k}$ is the number of intervals that have been opened at or before $b_{k}$.\n\\newline\n\\newline\nFinally, we define\n\\begin{equation*}\n   c_{j}^{(k)} = P_{0}(S_{k} = j \\textrm{ and } l_{q} \\leq S_{q} \\leq u_{q} \\textrm{ for } q = 1, ..., k - 1), \\textrm{for } k = 1, ..., 2n \\textrm{ and } j = 1, ..., n.\n\\end{equation*}\nBelow is the pseudocode for step 1. The algorithm calculates the probability of a valid allocation (under the null hypothesis) of points in bins by looping over each bin and making a recursive calculation.\n% This is where I want to put the algorithm, and then I will include the derivations of important terms below this.\n% There is something very strange with the spacing here that needs to be fixed I can't write text above the algorithm and the distance between the algorithm and the figure is too large. I also may want to add a few comments to the algorithm\n\\begin{algorithm}[H]\n\\caption{Calculate Type I error $\\alpha$ from proposed rejection region}\n\\textbf{Input:} Vector of lower bound values $(h_{1}, ..., h_{n})$, vector of upper bound values $(g_{1}, ..., g_{n})$.\n\\newline\n\\textit{get\\_level\\_from\\_bounds\\_two\\_sided}$(h_{1}, ..., h_{n}, g_{1}, ..., g_{n})$\n\\begin{algorithmic}[1]\n\\STATE $b_{1}, ..., b_{2n} \\leftarrow sort(h_{1}, ..., h_{n}, g_{1}, ..., g_{n})$\n\\STATE {$c_{0}^{(1)} \\leftarrow (1 - b_{1})^{n}$}\n\\STATE $l_{1} \\leftarrow 0$\n\\STATE $u_{1} \\leftarrow 0$\n\\FOR{$k = 2, ..., 2n$}\n\\IF{$b_{k - 1} \\in \\{h_{1}, ..., h_{n}\\}$}\n\\STATE $u_{k} \\leftarrow u_{k - 1} + 1$\n\\ENDIF\n\\IF{$b_{k} \\in \\{g_{1}, ..., g_{n}\\}$}\n\\STATE $l_{k} \\leftarrow l_{k - 1} + 1$\n\\ENDIF\n\\FOR{$j = l_{k}, ..., u_{k}$}\n\\STATE $c_{j}^{(k)} \\leftarrow 0 $\n\\FOR{$m = l_{k - 1}, ..., min(u_{k - 1}, j)$}\n\\STATE $c_{j}^{(k)} \\leftarrow  c_{j}^{(k)} + c_{m}^{(k - 1)} * dbinom(x = j - m, size = n-m, prob = \\frac{(b_{k} - b_{k - 1})}{(1 - b_{k - 1})})$\n\\ENDFOR\n\\ENDFOR\n\\ENDFOR\n\\RETURN $1 - c_{n}^{(2n)}$\n\\end{algorithmic}\n\\textbf{end}\n\\end{algorithm}\nWhere $dbinom(x = a, size = n, prob = p)$ denotes the probability mass function of a $Binomial(n, p)$ random variable evaluated at $a$.\n\\newline\n\\newline\n% Note, need to change all of the l values below to m's to match what the algorithm says above\n% Also need to finish writing out the algorithm\n% I can do all of this either tonight or tomorrow at some point\n% Then I also need to get the bibliography and to the pre-computation for the graphs.\nNow, we derive the algorithm. \\textbf{Initialization:} (Lines 2 through 4 above)\n\\begin{align*}\n    c_{0}^{(1)} &= P_{0}(S_{1} = 0)\\\\\n    &= P_{0}\\Big(\\bigcap\\limits_{i=1}^{n}\\{X_{i} \\in (b_{1}, 1)\\}\\Big)\\\\\n    &= \\prod\\limits_{i=1}^{n}P_{0}\\Big(X_{i} \\in (b_{1}, 1)\\Big) &\\textrm{(by independence)}\\\\\n    &= (1 - b_{1})^{n} &\\textrm{(Since each $X_{i} \\sim~ Unif(0, 1)$ under the null)}\n\\end{align*}\nAlso, we initialize $l_{1} = 0$ and $u_{1} = 0$ because the only allowable partial sum is 0 in the first bin.\n\\textbf{Recursion:} (Line 15 above)\n\\begin{flalign*}\n    \\indent\\indent c_{j}^{(k)} &= P_{0}(S_{k} = j \\textrm{ and } l_{q} \\leq S_{q} \\leq u_{q} \\textrm{ for } q = 1, ..., k - 1)\\\\\n    &= P_{0}\\Big(\\bigcup\\limits_{m=l_{k- 1}}^{min(j, u_{k- 1})} \\{S_{k - 1} = m \\textrm{ and } N_{k} = j - m\\} \\textrm{ and } l_{q} \\leq S_{q} \\leq u_{q} \\textrm{ for } q = 1, ..., k - 2\\Big)\\\\\n    &= \\sum\\limits_{m=l_{k- 1}}^{min(j, u_{k- 1})}P_{0}(S_{k - 1} = m \\textrm{ and } N_{k} = j - m \\textrm{ and } l_{q} \\leq S_{q} \\leq u_{q} \\textrm{ for } q = 1, ..., k - 2)\\\\\n    &= \\sum\\limits_{m=l_{k- 1}}^{min(j, u_{k- 1})}P_{0}(S_{k - 1} = m \\textrm{ and } l_{q} \\leq S_{q} \\leq u_{q} \\textrm{ for } q = 1, ..., k - 2) \\cdot\\\\\n    &P_{0}(N_{k} = j - m | S_{k - 1} = m \\textrm{ and } l_{q} \\leq S_{q} \\leq u_{q} \\textrm{ for } q = 1, ..., k - 2)\\\\\n    &= \\sum\\limits_{m=l_{k- 1}}^{min(j, u_{k- 1})}c_{m}^{(k - 1)} * P_{0}(N_{k} = j - m | S_{k - 1} = m)\\\\\n    &= \\sum\\limits_{m=l_{k- 1}}^{min(j, u_{k- 1})}c_{m}^{(k - 1)} * P(B = j - m), \\textrm{where }B \\sim Binomial\\left(n - m, \\frac{(b_{k} - b_{k - 1})}{(1 - b_{k - 1})}\\right),\n    &&\n\\end{flalign*}\n\\newline\n\\indent\\indent which is easily computed.\n\\newline\n\\newline\n% The step below should definitely be put in an algorithm float as well. I need to make sure all of this makes sense before I finish it, but afterh this the discussion and the abstract should be relatively easy to follow through on and complete.\n\\textbf{Step 2:} This step is done with a simple binary search over $\\eta$. We begin by setting $\\eta_{upper} = \\frac{-log(1 - \\alpha)}{(2 * log(log(n)) * log(n))}$ since this is an upper bound on the local level as shown by Gontscharuk et al. (2016), and we set $\\eta_{lower} = \\frac{\\alpha}{n}$, as this is the lower bound given by the Bonferroni correction. The pseudocode is shown below:\n\\begin{algorithm}[H]\n\\caption{Calculate testing bounds from global level $\\alpha$ and sample size $n$}\n\\textbf{Input:} Local level $\\alpha$, sample size $n$, tolerance $\\epsilon$\n\\newline\n\\textit{get\\_bounds\\_two\\_sided}$(\\alpha, n, \\epsilon)$\n\\begin{algorithmic}[1]\n\\STATE $\\eta_{upper} \\leftarrow \\frac{-log(1 - \\alpha)}{(2 * log(log(n)) * log(n))}$\n\\STATE $\\eta_{lower} \\leftarrow \\frac{\\alpha}{n}$\n\\STATE $\\alpha_{mid} \\leftarrow \\infty$\n\\WHILE{$\\frac{\\alpha_{mid} - \\alpha}{\\alpha} > \\epsilon$}\n\\STATE $\\eta_{mid} \\leftarrow \\frac{\\eta_{upper} + \\eta_{lower}}{2}$\n\\STATE $h_{1}, ..., h_{n} \\leftarrow qbeta(p = \\frac{\\eta_{mid}}{2}$, shape1 = c(1:n), shape2 = c(n:1))\n\\STATE $g_{1}, ..., g_{n} \\leftarrow qbeta(p = 1 - \\frac{\\eta_{mid}}{2}$, shape1 = c(1:n), shape2 = c(n:1))\n\\STATE $\\alpha_{mid} \\leftarrow get\\_level\\_from\\_bounds\\_two\\_sided(h_{1}, ..., h_{n}, g_{1}, ..., g_{n})$\n\\IF{$\\alpha_{mid} > \\alpha$}\n\\STATE $\\eta_{upper} \\leftarrow \\eta_{mid}$\n\\ELSE\n\\STATE $\\eta_{lower} \\leftarrow \\eta_{mid}$\n\\ENDIF\n\\ENDWHILE\n\\RETURN $h_{1}, ..., h_{n}, g_{1}, ..., g_{n}$\n\\end{algorithmic}\n\\textbf{end}\n\\end{algorithm}\nWhere $qbeta(p = x, shape1 = \\alpha, shape2 = \\beta)$ is the inverse cdf of a $Beta(\\alpha, \\beta)$ random variable evaluated at $x$ ($F^{-1}(x)$).\n\\section{Examples}\nAs noted above, the algorithm presented is only correct if all of the parameters of $F_{0}$ are known, and all $X_{1}, ..., X_{n}$ are independent. While both of these assumptions may only hold in a small number of cases, the methods above are still useful in a variety of scenarios.\n\\newline\n\\newline\nOne of the main advantages of the local levels method, is that it can easily be used to put testing bands onto QQplots by simply graphing each $(h_{i}, g_{i})$ interval. This allows us to examine how a dataset might deviate from some null distribution much better than simply applying a test that yields a binary conclusion. Below, we present a few examples where a QQplot is useful, and where the local levels test seems ideal for assessing deviation from a global null hypothesis.\n\n\\subsection{Examining the P-value Distribution for Testing Procedures}\n\n% It may also make sense for me to make a comparison to the Kolmogorov-Smirnov statistic at some point. I should probably include some instructive graphs or something.\n% I want to have a different paragraph to better introduce the concept\n% This shouldn't be too difficult\nSuppose we have devised a new testing procedure to test a null hypothesis $H_{0}$ with test statistic $T(X_{1}, ..., X_{n})$, and we wish to establish that this test has correct Type I error rate.\n\\newline\n\\newline\nTypically, the verification of Type I error rate is done using the following procedure:\n\\newline\n\\newline\n(1) Generate $m$ simulated datasets: $(X_{1}^{(1)}, ..., X_{n}^{(1)}), ..., (X_{1}^{(m)}, ..., X_{n}^{(m)})$ under $H_{0}$.\n\\newline\n\\newline\n(2) Apply the test $T$ to each of these datasets at level $\\alpha$ to yield $m$ reject / not-reject conclusions: $c_{1}, ..., c_{m}$, where $c_{i} = 1$ if $H_{0}$ is rejected and $0$ otherwise.\n\\newline\n\\newline\n(3) Conduct a test to confirm that the mean of $c_{1}, ..., c_{m}$ is $\\alpha$.\n\\newline\n\\newline\nWhile the above procedure provides reliable information about the Type I error calibration for one level of $\\alpha$, it provides little information about the global calibration of p-values. Instead, we suggest the following procedure:\n\\newline\n\\newline\n(1) As above.\n\\newline\n\\newline\n(2) Apply the test $T$ to each of these datasets to yield $m$ p-values: $p_{1}, ..., p_{m}$.\n\\newline\n\\newline\n(3) Make a QQplot of $p_{1}, ..., p_{m}$ with simultaneous testing bands from the local levels procedure to test if $p_{1}, ..., p_{m} \\distras{iid} Unif(0, 1)$.\n% It seems to me that this would be an opportune time to make a comparison to the KS-test, but I'm not totally sure.\n% I should maybe put some graphs here as well.\n\\newline\n\\newline\nThis allows us to easily visualize the global calibration of the p-values with just one graph and diagnose any issues if they exist. In Step 3, one could use one of various alternative. However, in the calibration of p-values, we typically do not have the expectation that our p-values would be more likely to deviate from uniform in some regions more than others, and so it makes sense to use the local levels test because it is agnostic to the alternative distribution. Moreover, since it is generally most concerning if p-values are not calibrated in the lower tail of the distribution, the local levels test is preferable to the standard KS test because it is much more sensitive in the tails (Aldor-Noiman 2013).\n\n\\subsection{Multiple Testing in Genome Wide Association Studies}\nIn genome wide association studies (GWAS), we are interested in identifying the genetic variants that are responsible for biological traits. Often, we test a large number of single-nucleotide polymorphisms (SNPs), to see if any of them are strongly correlated with some discrete or continuous trait. Often, it is useful to test the following global null hypothesis:\n\\newline\n\\newline\n$H_{0}$: All genome-wide SNPs in some functional category have correlation 0 with the trait of interest.\n\n\\begin{equation*}\n\\textrm{ vs. }\n\\end{equation*}\n\n\\noindent$H_{A}$: At least one SNP in some functional category has non-zero correlation with the trait of interest.\n\\newline\n\\newline\nTypically, when testing this global null, we apply some test to each of $n$ SNPs, yielding $n$ p-values. These p-values are often put on a QQplot for the uniform distribution, and either no bands, pointwise bands, or occasionally KS bands are put on the plot. While the SNPs have a local correlation structure, this has only a weak effect at a genome-wide scale. Thus, the local levels procedure still provides a valuable alternative visualization tool to assess the extent and type of any deviation from the null, even though it is not exact. Particularly in genetics, if the p-values deviate from the null, it can be very useful to view graphically how they deviate. For instance, if there are a few very large p-values but the rest of the distribution looks relatively uniform, then this indicates that the trait of interest is driven by a few mutations. If, however, there are some small deviations throughout the p-value distribution, then this indicates that trait of interest is driven by a number of genes that all play some small part in a complex biological process, or potentially by some confounding variables that are not controlled for.\n\\newline\n\\newline\nIf a very specific distribution for $H_{A}$ is suspected, then it makes sense to use a test that has high power under this alternative, which may not be our local levels test. However, in the general case when little is known about the alternative distribution, using local levels is desirable because it is agnostic to alternative distributions. Moreover, since small p-values are often of great interest in GWAS because they can indicate the main biological drivers of a trait, the local levels test is far superior to the KS test due to its tail sensitivity.\n\\newline\n\\newline\nAs an example, we simulate data from the following model and demonstrate our bands on the data. Let $Y_{i}$ be a continuous biological trait of interest for individual $i$, $i = 1, ..., n$. Let $G_{ij}$ be the allele value for individual $i$ at SNP $j$, $j = 1, ..., m$. If $f_{j}$ is the allele frequency at SNP $j$ for the population, then $\\frac{G_{ij} - 2f_{j}}{\\sqrt{2f_{j}(1 - f_{j})}}$ is the standardized allele value for individual $i$ at SNP $j$. Finally, let $\\alpha_{j}$ be the fixed effect of the standardized allele value at SNP $j$ on the outcome $Y$. Thus, we generate data from the model:\n\\begin{align*}\n&Y_{i} = \\sum_{j = 1}^{m}\\alpha_{j}\\frac{G_{ij} - 2f_{j}}{\\sqrt{2f_{j}(1 - f_{j})}} + \\epsilon_{i} \\\\\n&f_{1}, ..., f_{m} \\distras{iid} Unif(.05, .5) \\\\\n&\\epsilon_{1}, ..., \\epsilon_{n} \\distras{iid} N(0, 1) \\\\\n&\\alpha_{1}, ..., \\alpha_{m} \\distras{iid} \\pi N(0, \\sigma_{A}^{2}) + (1 - \\pi) \\delta_{0}\n\\end{align*}\nA function to generate data from this model is shown below, implemented in R:\n%\n<<sim_data>>=\nsim_from_genetic_model <- function(num_ppl,\n                                   num_genes,\n                                   prop_true,\n                                   signal_var,\n                                   noise_var) {\n\n  # Generate normal signals for the genes related to the outcome\n  true_vec <- rnorm(n = num_genes, mean = 0, sd = sqrt(signal_var))\n  sigs <- runif(n = num_genes)\n  sig_true <- as.numeric(sigs <= prop_true)\n  signal_vec <- true_vec * sig_true\n\n  # Generate allele frequencies\n  total_data <- rbinom(n = num_ppl * num_genes, size = 2, prob = .3)\n  G_mat <- matrix(data = total_data, nrow = num_ppl, ncol = num_genes)\n  signal_col <- matrix(data = signal_vec, ncol = 1)\n\n  # generate outcome with added normal noise\n  Y_vec <- G_mat %*% signal_col +\n    rnorm(n = num_ppl, mean = 0, sd = sqrt(noise_var))\n\n  return(list(Y = Y_vec, G = G_mat))\n\n}\n@\n%\nNow, we calculate the bounds for the local levels test using our package:\n%\n<<get_bounds, eval = FALSE>>=\nlibrary(qqconf)\nbounds <- get_bounds_two_sided(alpha = .05, n = 500000)\n@\n%\n\\section{Discussion}\nThe local levels test using an exact recursive calculation allows for the creation of relatively fast and alternative-agnostic simultaneous testing bands for QQplots. Of course, our method is limited by the assumptions that (1) all of the tests are independent and that (2) all of the parameters of the null distribution are known. Currently, we are not aware of methods to handle the dependent case that allow for bands on QQplots. To handle the second issue, Aldor-Noiman et al. (2013) have adapted their method such that it will yield approximately correct results when some parameters of the null distribution are unknown. However, if the dataset is even moderately large, this method is prohibitively slow. In this case, we suggest that our method be used because it is much faster, and with a sufficiently large dataset the uncertainty in the parameters will become negligible under standard regularity conditions. Methods to handle parameter uncertainty with an exact calculation have been discussed in the context of the normal distribution \\citep{rosenkrantz2000confidence}, but a general method towards this end has not been developed, and would be an important next step.\n\\newline\n\\newline\nOur software is available on CRAN. To aid in computation speed, we have pre-computed a number of different local level values given a global level and the size of the dataset. This computes the testing bands very quickly with little error. We have also implemented a one-sided test for the case in which we are only concerned with values that are smaller than would be expected under the null (e.g. we only care about small p-values).\n\n\\bibliography{refs}\n%\\bibliographystyle{ieeetr}\n\n\\end{document}\n",
    "created" : 1562348963258.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "4275985860",
    "id" : "4A5F1E77",
    "lastKnownWriteTime" : 1562559390,
    "last_content_update" : 1562559391015,
    "path" : "~/Downloads/jss-article-rnw (2)/article.Rnw",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 3,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "sweave"
}