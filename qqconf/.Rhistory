?rbeta
Aa <- c(1, 2)
AA <- c(3, 4)
?sample
sample(c(0, 1), 1, prob = c(.1, .9))
sample(c(0, 1), 1, prob = c(.1, .9))
sample(c(0, 1), 1, prob = c(.1, .9))
sample(c(0, 1), 1, prob = c(.1, .9))
sample(c(0, 1), 1, prob = c(.1, .9))
sample(c(0, 1), 1, prob = c(.1, .9))
sample(c(0, 1), 1, prob = c(.1, .9))
sample(c(0, 1), 1, prob = c(.1, .9))
sample(c(0, 1), 1, prob = c(.1, .9))
sample(c(0, 1), 1, prob = c(.1, .9))
sample(c(0, 1), 1, prob = c(.1, .9))
sample(c(0, 1), 1, prob = c(.1, .9))
sample(c(0, 1), 1, prob = c(.1, .9))
sample(c(0, 1), 1, prob = c(.1, .9))
sample(c(0, 1), 1, prob = c(.1, .9))
sample(c(0, 1), 1, prob = c(.1, .9))
sample(c(0, 1), 1, prob = c(.1, .9))
sample(c(0, 1), 1, prob = c(.1, .9))
sample(c(0, 1), 1, prob = c(.1, .9))
sample(c(0, 1), 1, prob = c(.1, .9))
sample(c(0, 1), 1, prob = c(.1, .9))
sample(c(0, 1), 1, prob = c(.1, .9))
sample(c(0, 1), 1, prob = c(.1, .9))
sample(c(0, 1), 1, prob = c(.1, .9))
sample(c(0, 1), 1, prob = c(.5, .5))
sample(c(0, 1), 1, prob = c(.5, .5))
sample(c(0, 1), 1, prob = c(.5, .5))
sample(c(0, 1), 1, prob = c(.5, .5))
sample(c(0, 1), 1, prob = c(.5, .5))
sample(c(0, 1), 1, prob = c(.5, .5))
sample(c(0, 1), 1, prob = c(.5, .5))
sample(c(0, 1), 1, prob = c(.5, .5))
sample(c(0, 1), 1, prob = c(.5, .5))
sample(c(0, 1), 1, prob = c(.5, .5))
?scale
?sample
knitr::opts_chunk$set(echo = TRUE)
# First, want to simulate data with these parameters
set.seed(33)
# generate from mixture of normals
#' @param n number of samples
#' @param pi mixture proportions
#' @param mu mixture means
#' @param s mixture standard deviations
rmix = function(n,pi,mu,s){
z = sample(1:length(pi),prob=pi,size=n,replace=TRUE)
x = rnorm(n,mu[z],s[z])
return(x)
}
x = rmix(n=1000,pi=c(0.2,0.8),mu=c(-2,2),s=c(1,1))
# Now, apply the Gibb's Sampler to this data
x = rmix(n=1000,pi=c(0.2,0.8),mu=c(-2,2),s=c(1,1))
res <- gibbs(x,2,niter =1000)
normalize = function(x){return(x/sum(x))}
#' @param x an n vector of data
#' @param pi a k vector
#' @param mu a k vector
sample_z = function(x,pi,mu){
dmat = outer(mu,x,"-") # k by n matrix, d_kj =(mu_k - x_j)
p.z.given.x = as.vector(pi) * dnorm(dmat,0,1)
p.z.given.x = apply(p.z.given.x,2,normalize) # normalize columns
z = rep(0, length(x))
for(i in 1:length(z)){
z[i] = sample(1:length(pi), size=1,prob=p.z.given.x[,i],replace=TRUE)
}
return(z)
}
#' @param z an n vector of cluster allocations (1...k)
#' @param k the number of clusters
sample_pi = function(z,k){
counts = colSums(outer(z,1:k,FUN="=="))
pi = gtools::rdirichlet(1,counts+.5)
return(pi)
}
#' @param x an n vector of data
#' @param z an n vector of cluster allocations
#' @param k the number o clusters
#' @param prior.mean the prior mean for mu
#' @param prior.prec the prior precision for mu
sample_mu = function(x, z, k, prior){
df = data.frame(x=x,z=z)
mu = rep(0,k)
for(i in 1:k){
sample.size = sum(z==i)
sample.mean = ifelse(sample.size==0,0,mean(x[z==i]))
post.prec = sample.size+prior$prec
post.mean = (prior$mean * prior$prec + sample.mean * sample.size)/post.prec
mu[i] = rnorm(1,post.mean,sqrt(1/post.prec))
}
return(mu)
}
gibbs = function(x,k,niter =1000,muprior = list(mean=0,prec=0.1)){
pi = rep(1/k,k) # initialize
mu = rnorm(k,0,10)
z = sample_z(x,pi,mu)
res = list(mu=matrix(nrow=niter, ncol=k), pi = matrix(nrow=niter,ncol=k), z = matrix(nrow=niter, ncol=length(x)))
res$mu[1,]=mu
res$pi[1,]=pi
res$z[1,]=z
for(i in 2:niter){
pi = sample_pi(z,k)
mu = sample_mu(x,z,k,muprior)
z = sample_z(x,pi,mu)
res$mu[i,] = mu
res$pi[i,] = pi
res$z[i,] = z
}
return(res)
}
# First, want to simulate data with these parameters
set.seed(33)
# generate from mixture of normals
#' @param n number of samples
#' @param pi mixture proportions
#' @param mu mixture means
#' @param s mixture standard deviations
rmix = function(n,pi,mu,s){
z = sample(1:length(pi),prob=pi,size=n,replace=TRUE)
x = rnorm(n,mu[z],s[z])
return(x)
}
# Now, apply the Gibb's Sampler to this data
x = rmix(n=1000,pi=c(0.2,0.8),mu=c(-2,2),s=c(1,1))
res <- gibbs(x,2,niter =1000)
plot(res$pi[,1],ylim=c(0,1),type="l", ylab = "$\pi$")
plot(res$pi[,1],ylim=c(0,1),type="l", ylab = "pi")
# Now, apply the Gibb's Sampler to this data
x = rmix(n=1000,pi=c(0.2,0.8),mu=c(-2,2),s=c(1,1))
res <- gibbs(x,2,niter =1000)
plot(res$pi[,1],ylim=c(0,1),type="l", ylab = "pi", xlab = "Sample Index", main = "n = 1000")
hist(res$pi[,1], xlab = "pi", freq = FALSE, main = "n = 1000")
# Now, apply the Gibb's Sampler to this data
x = rmix(n=1000,pi=c(0.2,0.8),mu=c(-2,2),s=c(1,1))
res <- gibbs(x,2,niter =1000)
plot(res$pi[,1],ylim=c(0,1),type="l", ylab = "pi", xlab = "Sample Index", main = "n = 1000")
hist(res$pi[,1], xlab = "pi", freq = FALSE, main = "n = 1000", breaks = seq(from = 0, to = 1, by = .1))
plot(res$pi[,1],ylim=c(0,1),type="l", ylab = "pi", xlab = "Sample Index", main = "n = 1000")
hist(res$pi[,1], xlab = "pi", freq = FALSE, main = "n = 1000", breaks = seq(from = 0, to = 1, by = .075))
hist(res$pi[,1], xlab = "pi", freq = FALSE, main = "n = 1000", breaks = seq(from = 0, to = 1, length.out = 13))
hist(res$pi[,1], xlab = "pi", freq = FALSE, main = "n = 1000", breaks = seq(from = 0, to = 1, length.out = 15))
hist(res$pi[,1], xlab = "pi", freq = FALSE, main = "n = 1000", breaks = seq(from = 0, to = 1, length.out = 20))
# First, want to simulate data with these parameters
set.seed(33)
# generate from mixture of normals
#' @param n number of samples
#' @param pi mixture proportions
#' @param mu mixture means
#' @param s mixture standard deviations
rmix = function(n,pi,mu,s){
z = sample(1:length(pi),prob=pi,size=n,replace=TRUE)
x = rnorm(n,mu[z],s[z])
return(x)
}
# Now, apply the Gibb's Sampler to this data
x = rmix(n=1000,pi=c(0.2,0.8),mu=c(-2,2),s=c(1,1))
res <- gibbs(x,2,niter =1000)
plot(res$pi[,1],ylim=c(0,1),type="l", ylab = "pi", xlab = "Sample Index", main = "n = 1000")
hist(res$pi[,1], xlab = "pi", freq = FALSE, main = "n = 1000", breaks = seq(from = 0, to = 1, length.out = 20))
x = rmix(n=100,pi=c(0.2,0.8),mu=c(-2,2),s=c(1,1))
res <- gibbs(x,2,niter =1000)
plot(res$pi[,1],ylim=c(0,1),type="l", ylab = "pi", xlab = "Sample Index", main = "n = 100")
hist(res$pi[,1], xlab = "pi", freq = FALSE, main = "n = 100", breaks = seq(from = 0, to = 1, length.out = 20))
x = rmix(n=10,pi=c(0.2,0.8),mu=c(-2,2),s=c(1,1))
res <- gibbs(x,2,niter =1000)
plot(res$pi[,1],ylim=c(0,1),type="l", ylab = "pi", xlab = "Sample Index", main = "n = 10")
hist(res$pi[,1], xlab = "pi", freq = FALSE, main = "n = 1000", breaks = seq(from = 0, to = 1, length.out = 20))
# First, want to simulate data with these parameters
set.seed(33)
# generate from mixture of normals
#' @param n number of samples
#' @param pi mixture proportions
#' @param mu mixture means
#' @param s mixture standard deviations
rmix = function(n,pi,mu,s){
z = sample(1:length(pi),prob=pi,size=n,replace=TRUE)
x = rnorm(n,mu[z],s[z])
return(x)
}
# Now, apply the Gibb's Sampler to this data
x = rmix(n=1000,pi=c(0.2,0.8),mu=c(-2,2),s=c(1,1))
res <- gibbs(x,2,niter =1000)
plot(res$pi[,1],ylim=c(0,1),type="l", ylab = "pi", xlab = "Sample Index", main = "n = 1000")
hist(res$pi[,1], xlab = "pi", freq = FALSE, main = "n = 1000", breaks = seq(from = 0, to = 1, length.out = 20))
x = rmix(n=100,pi=c(0.2,0.8),mu=c(-2,2),s=c(1,1))
res <- gibbs(x,2,niter =1000)
plot(res$pi[,1],ylim=c(0,1),type="l", ylab = "pi", xlab = "Sample Index", main = "n = 100")
hist(res$pi[,1], xlab = "pi", freq = FALSE, main = "n = 100", breaks = seq(from = 0, to = 1, length.out = 20))
x = rmix(n=10,pi=c(0.2,0.8),mu=c(-2,2),s=c(1,1))
res <- gibbs(x,2,niter =1000)
plot(res$pi[,1],ylim=c(0,1),type="l", ylab = "pi", xlab = "Sample Index", main = "n = 10")
hist(res$pi[,1], xlab = "pi", freq = FALSE, main = "n = 10", breaks = seq(from = 0, to = 1, length.out = 20))
setwd("~/Documents")
warnings()
schools_df
# This function just needs to give the joint density of mu, eta, and x
# Then I can plug this into the MH algorithm without knowing the normalizing constant
log_lik_normal_hier <- function(x_vec, s_vec, mu, eta, a = 10 ^ 6, b = 10) {
if(abs(mu) > a | abs(eta) > b){
return(-Inf)
}
sigma = exp(eta)
n <- length(x_vec)
log_lik = 0
for(i in seq(from = 1, to = n, by = 1)) {
log_lik <- log_lik - (log(sqrt(2 * pi * (sigma ^ 2 + s_vec[i] ^ 2))) + ((x_vec[i] - mu) ^ 2) / (2 * (sigma ^ 2 + s_vec[i] ^ 2)))
}
return(log_lik)
}
# I should set this up to be very similar to what I did above, except I need to change the prior and the liklihood.
# That should be it though
mh_normal_means <- function(num_iter = 1000, mu_start, eta_start, mu_sd, eta_sd, x_vec, s_vec) {
mu_samp <- numeric(num_iter)
eta_samp <- numeric(num_iter)
mu_samp[1] <- mu_start
eta_samp[1] <- eta_start
for(i in seq(from = 2, to = num_iter, by = 1)) {
current_mu <- mu_samp[i - 1]
current_eta <- eta_samp[i - 1]
new_mu <- current_mu + rnorm(n = 1, mean = 0, sd = mu_sd)
new_eta <- current_eta + rnorm(n = 1, mean = 0, sd = eta_sd)
A_mu <- exp(log_lik_normal_hier(x_vec = x_vec, s_vec = s_vec, mu = new_mu, eta = current_eta) -
log_lik_normal_hier(x_vec = x_vec, s_vec = s_vec, mu = current_mu, eta = current_eta) )
if(runif(1) < A_mu) {
mu_samp[i] <- new_mu
}
A_eta <- exp(log_lik_normal_hier(x_vec = x_vec, s_vec = s_vec, mu = mu_samp[i], eta = new_eta) -
log_lik_normal_hier(x_vec = x_vec, s_vec = s_vec, mu = mu_samp[i], eta = current_eta) )
if(runif(1) < A_eta) {
eta_samp[i] <- new_eta
}
}
return(list(mu = mu_samp, eta = eta_samp))
}
# Want to simulate a bunch of data
# To do this, first simulate from a normal with mu and eta a value
# Then simulate the data x using the sd vec
par(mfrow = c(1,1))
set.seed(348)
mu <- 0
eta <- log(1)
num_points <- 100
init_seq <- list(c(-50, 1.3), c(-3, 4), c(4, .2), c(12, .75))
for(init in init_seq) {
# First, generate the data
s_vec <- runif(n = num_points, min = .75, max = 1.25)
mu_vec <- rnorm(n = num_points, mean = mu, sd = exp(eta))
x_vec <- rnorm(n = num_points, mean = mu_vec, sd = s_vec)
# Now, need to run the metropolis hastings
out_dist <- mh_normal_means(mu_start = init[1], eta_start = init[2], mu_sd = .1,
eta_sd = .01, x_vec = x_vec, s_vec = s_vec)
log_lik_seq <- numeric(1000)
for(i in seq(from = 1, to = 1000, by = 1)) {
log_lik_seq[i] <- log_lik_normal_hier(x_vec = x_vec, s_vec = s_vec, mu = out_dist$mu[i], eta = out_dist$eta[i])
}
plot(log_lik_seq, type = "l", ylab = "Log Liklihood", main = glue::glue("Plot for All Indices - mu_start = {init[1]}, sigma_start = {round(exp(init[2]), 2)}"))
plot(log_lik_seq[26:1000], type = "l", ylab = "Log Liklihood", main = glue::glue("Plot Excluding First 25 Samples - mu_start = {init[1]}, sigma_start = {round(exp(init[2]), 2)}"))
hist(out_dist$mu[26:1000], freq = FALSE, xlab = "Mu", main = glue::glue("Mu Posterior: mean = {round(mean(out_dist$mu[26:1000]), 3)}, sd = {round(sd(out_dist$mu[26:1000]), 3)}, 90% CI = [{round(as.numeric(quantile(out_dist$mu[26:1000], c(.05))), 3)}, {round(as.numeric(quantile(out_dist$mu[26:1000], c(.95))), 3)}]"))
hist(exp(out_dist$eta[26:1000]), freq = FALSE, xlab = "Sigma", main = glue::glue("Sigma Posterior: mean = {round(mean(exp(out_dist$eta[26:1000])), 3)}, sd = {round(sd(exp(out_dist$eta[26:1000])), 3)}, 90% CI = [{round(as.numeric(quantile(exp(out_dist$eta[26:1000]), c(.05))), 3)}, {round(as.numeric(quantile(exp(out_dist$eta[26:1000]), c(.95))), 3)}]"))
# I'll worry about the histograms later, but for now I'll focus on the plots I'm instructed to make
#hist(out_dist$mu)
#hist(exp(out_dist$eta))
}
schools_df <- readr::read_csv("~/Documents/8_schools.csv")
schools_out <- mh_normal_means(num_iter = 100000, mu_start = 0, eta_start = 0, mu_sd = .1, eta_sd = .01, x_vec = schools_df$treatment_effect, s_vec = schools_df$se)
hist(schools_out$mu, xlab = "Mu", freq = FALSE, main = "Histogram of Mu - 100k Samples")
hist(exp(schools_out$eta), xlab = "Sigma", freq = FALSE, main = "Histogram of Sigma - 100k Samples")
te_mean <- mean(schools_out$mu)
te_sd <- sd(schools_out$mu)
te_ci <- quantile(schools_out$mu, probs = c(.05, .95))
sigma_mean <- mean(exp(schools_out$eta))
sigma_sd <- sd(exp(schools_out$eta))
sigma_ci <- quantile(exp(schools_out$eta), probs = c(.05, .95))
# First, I need to figure out what the form of the posterior is here
# This is jut the simple result from the normal - normal model with known variance
# I'll look this up now and then I should be good
# It's weird that we wouldn't just use the actual variance formula, but this is fine too
# I know what the setup is and it should work
schools_out$mu
get_school_tf <- function(mh_out, s_vec, te_vec, school_num) {
num_sims <- length(mh_out$mu)
num_schools <- length(s_vec)
mu <- mh_out$mu
eta <- mh_out$eta
theta_post <- numeric(num_sims)
for(i in seq(from = 1, to = num_sims, by = 1)) {
theta_post[i] <- ((mu[i] / (exp(eta[i]) ^ 2)) + (te_vec[school_num] / (s_vec[school_num] ^ 2))) /
((1 / (exp(eta[i]) ^ 2)) + (1 / (s_vec[school_num] ^ 2)))
}
return(theta = theta_post)
}
# First, I need to figure out what the form of the posterior is here
# This is jut the simple result from the normal - normal model with known variance
# I'll look this up now and then I should be good
# It's weird that we wouldn't just use the actual variance formula, but this is fine too
# I know what the setup is and it should work
school_vec <- seq(from = 1, to = 8, by = 1)
for(school in school_vec) {
tf_out <- get_school_tf(schools_out, schools_df$se, schools_df$treatment_effect, school)
}
tf_out
# First, I need to figure out what the form of the posterior is here
# This is jut the simple result from the normal - normal model with known variance
# I'll look this up now and then I should be good
# It's weird that we wouldn't just use the actual variance formula, but this is fine too
# I know what the setup is and it should work
tf_out_1 <- get_school_tf(schools_out, schools_df$se, schools_df$treatment_effect, 1)
tf_out_2 <- get_school_tf(schools_out, schools_df$se, schools_df$treatment_effect, 2)
tf_out_3 <- get_school_tf(schools_out, schools_df$se, schools_df$treatment_effect, 3)
tf_out_4 <- get_school_tf(schools_out, schools_df$se, schools_df$treatment_effect, 4)
tf_out_5 <- get_school_tf(schools_out, schools_df$se, schools_df$treatment_effect, 5)
tf_out_6 <- get_school_tf(schools_out, schools_df$se, schools_df$treatment_effect, 6)
tf_out_7 <- get_school_tf(schools_out, schools_df$se, schools_df$treatment_effect, 7)
tf_out_8 <- get_school_tf(schools_out, schools_df$se, schools_df$treatment_effect, 8)
tf_out_1
# First, I need to figure out what the form of the posterior is here
# This is jut the simple result from the normal - normal model with known variance
# I'll look this up now and then I should be good
# It's weird that we wouldn't just use the actual variance formula, but this is fine too
# I know what the setup is and it should work
tf_out_1 <- get_school_tf(schools_out, schools_df$se, schools_df$treatment_effect, 1)
tf_out_2 <- get_school_tf(schools_out, schools_df$se, schools_df$treatment_effect, 2)
tf_out_3 <- get_school_tf(schools_out, schools_df$se, schools_df$treatment_effect, 3)
tf_out_4 <- get_school_tf(schools_out, schools_df$se, schools_df$treatment_effect, 4)
tf_out_5 <- get_school_tf(schools_out, schools_df$se, schools_df$treatment_effect, 5)
tf_out_6 <- get_school_tf(schools_out, schools_df$se, schools_df$treatment_effect, 6)
tf_out_7 <- get_school_tf(schools_out, schools_df$se, schools_df$treatment_effect, 7)
tf_out_8 <- get_school_tf(schools_out, schools_df$se, schools_df$treatment_effect, 8)
mean_schools <- c(mean(tf_out_1), mean(tf_out_2), mean(tf_out_3), mean(tf_out_4), mean(tf_out_5), mean(tf_out_6), mean(tf_out_7), mean(tf_out_8))
var_schools <- c(var(tf_out_1), var(tf_out_2), var(tf_out_3), var(tf_out_4), var(tf_out_5), var(tf_out_6), var(tf_out_7), var(tf_out_8))
schools_out_df <- data.frame(school = seq(from = 1, to = 8), mean_effect = mean_schools, var_effect = var_schools)
knitr::kable(schools_out_df)
devtools::document()
devtools::document()
?monte_carlo_two_sided
?qqconf::monte_carlo_two_sided
?devtools::document
devtools::document("qqconf")
getwd()
devtools::document("../qqconf")
?monte_carlo_two_sided
?qqconf::monte_carlo_two_sided
devtools::document("../qqconf")
devtools::document("../qqconf")
?monte_carlo_two_sided
devtools::document("../qqconf")
?monte_carlo_two_sided
?xgboost::agaricus.train
?xgboost::dimnames.xgb.DMatrix
?xgboost::xgb.cv
c(T, T) ||c(F, F)
devtools::document("../qqconf")
?monte_carlo_two_sided
devtools::document("../qqconf")
devtools::document("../qqconf")
?get_bounds_two_sided
devtools::document("../qqconf")
devtools::document("../qqconf")
?devtools::use_rcpp
?devtools::use_rcpp
install.packages("usethis")
usethis::use_rcpp(name = "jointlevel_two_sided.c")
usethis::use_c(name = "jointlevel_two_sided.c")
devtools::document("../qqconf/")
library(qqconf)
v <- get_bounds_two_sided(.05, 10)
library(qqconf)
v <- get_bounds_two_sided(.05, 10)
v
devtools::document("../qqconf/")
library(qqconf)
get_bounds_onesided(.05, 10)
library(qqconf)
library(qqconf)
?get_level_from_bounds_one_sided()
?get_level_from_bounds_one_sided(c(.1, .5))
get_level_from_bounds_one_sided(c(.1, .5))
devtools::document("../qqconf/")
library(qqconf)
get_level_from_bounds_one_sided(c(.1, .5))
get_level_from_bounds_one_sided(c(5))
get_level_from_bounds_one_sided(c(.5))
library(qqconf)
get_level_from_bounds_one_sided(c(.5))
get_level_from_bounds_two_sided(c(.5), c(.7))
get_level_from_bounds_two_sided(c(.5), c(.8))
